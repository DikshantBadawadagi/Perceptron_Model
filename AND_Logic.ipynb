{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN5BAO8VQu7vtgXBFyTNTyv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DikshantBadawadagi/Perceptron_Model/blob/main/AND_Logic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "IT-qSY9ST5Sz"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def unitStep(v):\n",
        "  if(v>0):\n",
        "    return 1\n",
        "  else:\n",
        "    return -1\n",
        ""
      ],
      "metadata": {
        "id": "aJ4QXMlmT_Jy"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def perceptronModel(x,w,b):\n",
        "  v = np.dot(x,w) + b\n",
        "  y = unitStep(v)\n",
        "  return y"
      ],
      "metadata": {
        "id": "CxE0vVmlUHQe"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def AND_logic(x):\n",
        "  w = np.array([1,1])\n",
        "  b = -1.5\n",
        "  return perceptronModel(x,w,b)"
      ],
      "metadata": {
        "id": "NnCFBUVYUdw-"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test1 = np.array([-1, -1])\n",
        "test2 = np.array([1, 1])\n",
        "test3 = np.array([-1, 1])\n",
        "test4 = np.array([1, -1])"
      ],
      "metadata": {
        "id": "CxRsT27sUrXv"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"AND({}, {}) = {}\".format(-1, -1, AND_logic(test1)))\n",
        "print(\"AND({}, {}) = {}\".format(1, 1, AND_logic(test2)))\n",
        "print(\"AND({}, {}) = {}\".format(-1, 1, AND_logic(test3)))\n",
        "print(\"AND({}, {}) = {}\".format(1, -1, AND_logic(test4)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GINXAz-gU891",
        "outputId": "69bf043d-bb59-4e38-e3a6-d3e60ed2f862"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AND(-1, -1) = -1\n",
            "AND(1, 1) = 1\n",
            "AND(-1, 1) = -1\n",
            "AND(1, -1) = -1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define Unit Step Function for bipolar output\n",
        "def unitStep(v):\n",
        "    return 1 if v >= 0 else -1\n",
        "\n",
        "# Perceptron Model\n",
        "def perceptronModel(x, w, b):\n",
        "    v = np.dot(w, x) + b\n",
        "    return unitStep(v)\n",
        "\n",
        "# AND Logic Function with initial weights and bias as 0\n",
        "def AND_logicFunction(x, w, b, target):\n",
        "    y = perceptronModel(x, w, b)\n",
        "\n",
        "    # Print current state\n",
        "    print(f\"Current Input: {x}, Output: {y}, Target: {target}, Weights: {w}, Bias: {b}\")\n",
        "\n",
        "    if y == target:\n",
        "        return w, b  # Return weights and bias if output matches target\n",
        "\n",
        "    # Update weights and bias based on error\n",
        "    learning_rate = 0.1\n",
        "    error = target - y\n",
        "\n",
        "    # Update weights and bias\n",
        "    w = w + learning_rate * error * x\n",
        "    b = b + learning_rate * error\n",
        "\n",
        "    # Recursive call until output matches target\n",
        "    return AND_logicFunction(x, w, b, target)\n",
        "\n",
        "# Testing the Perceptron Model for AND gate with recursive updates\n",
        "# Initialize weights and bias\n",
        "initial_weights = np.array([0.0, 0.0])\n",
        "initial_bias = 0.0\n",
        "\n",
        "# Test cases\n",
        "test_cases = [\n",
        "    (np.array([-1, -1]), -1),  # Target output -1\n",
        "    (np.array([1, 1]), 1),      # Target output 1\n",
        "    (np.array([-1, 1]), -1),    # Target output -1\n",
        "    (np.array([1, -1]), -1)     # Target output -1\n",
        "]\n",
        "\n",
        "# Run the tests\n",
        "for x, target in test_cases:\n",
        "    print(f\"\\nTesting Input: {x}, Target: {target}\")\n",
        "    final_weights, final_bias = AND_logicFunction(x, initial_weights, initial_bias, target)\n",
        "    print(f\"Final Weights: {final_weights}, Final Bias: {final_bias}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e2ksVjx0VAZ1",
        "outputId": "80106724-6c59-43ac-f4ba-4d1d0607e3c3"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Testing Input: [-1 -1], Target: -1\n",
            "Current Input: [-1 -1], Output: 1, Target: -1, Weights: [0. 0.], Bias: 0.0\n",
            "Current Input: [-1 -1], Output: -1, Target: -1, Weights: [0.2 0.2], Bias: -0.2\n",
            "Final Weights: [0.2 0.2], Final Bias: -0.2\n",
            "\n",
            "Testing Input: [1 1], Target: 1\n",
            "Current Input: [1 1], Output: 1, Target: 1, Weights: [0. 0.], Bias: 0.0\n",
            "Final Weights: [0. 0.], Final Bias: 0.0\n",
            "\n",
            "Testing Input: [-1  1], Target: -1\n",
            "Current Input: [-1  1], Output: 1, Target: -1, Weights: [0. 0.], Bias: 0.0\n",
            "Current Input: [-1  1], Output: -1, Target: -1, Weights: [ 0.2 -0.2], Bias: -0.2\n",
            "Final Weights: [ 0.2 -0.2], Final Bias: -0.2\n",
            "\n",
            "Testing Input: [ 1 -1], Target: -1\n",
            "Current Input: [ 1 -1], Output: 1, Target: -1, Weights: [0. 0.], Bias: 0.0\n",
            "Current Input: [ 1 -1], Output: -1, Target: -1, Weights: [-0.2  0.2], Bias: -0.2\n",
            "Final Weights: [-0.2  0.2], Final Bias: -0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def unitStep(v):\n",
        "  if(v > 0): return 1\n",
        "  elif (v == 0):\n",
        "    return 0\n",
        "  else: return -1\n",
        "\n",
        "def perceptronModel(w,x,b):\n",
        "  v  = np.dot(w,x) + b\n",
        "  y = unitStep(v)\n",
        "  return y\n",
        "\n",
        "def AND_logic(w, x_list, b_list, t_list):\n",
        "    all_matched = True\n",
        "\n",
        "    for i in range(len(x_list)):\n",
        "        x = x_list[i]\n",
        "        t = t_list[i]\n",
        "        b = b_list[i]\n",
        "\n",
        "        y = perceptronModel(w, x, b)\n",
        "        print(f\"Current Input: {x}, Output: {y}, Target: {t}, Weights: {w}, Bias: {b}\")\n",
        "\n",
        "        if y != t:\n",
        "            all_matched = False\n",
        "            alpha = 0.01\n",
        "\n",
        "            w = w + alpha * t * x\n",
        "            b_list[i] = b + alpha * t\n",
        "\n",
        "    if all_matched:\n",
        "        return w, b_list\n",
        "\n",
        "    return AND_logic(w, x_list, b_list, t_list)\n",
        "\n",
        "ini_weights = np.zeros(9)\n",
        "ini_biases = np.zeros(2)\n",
        "\n",
        "x1 = np.array([1, 1, 1, 0, 1, 0, 1, 1, 1])\n",
        "x2 = np.array([1, 1, 1, 1, 1, 1, 1, 0, 0])\n",
        "\n",
        "t1 = 1\n",
        "t2 = -1\n",
        "\n",
        "x_list = [x1, x2]\n",
        "t_list = [t1, t2]\n",
        "\n",
        "final_weights, final_biases = AND_logic(ini_weights, x_list, ini_biases, t_list)\n",
        "print(f\"Final Weights: {final_weights}, Final Biases: {final_biases}\")\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GDAYFwovYwpU",
        "outputId": "514d063f-95ce-4bee-db25-cadc30c59425"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current Input: [1 1 1 0 1 0 1 1 1], Output: 0, Target: 1, Weights: [0. 0. 0. 0. 0. 0. 0. 0. 0.], Bias: 0.0\n",
            "Current Input: [1 1 1 1 1 1 1 0 0], Output: 1, Target: -1, Weights: [0.01 0.01 0.01 0.   0.01 0.   0.01 0.01 0.01], Bias: 0.0\n",
            "Current Input: [1 1 1 0 1 0 1 1 1], Output: 1, Target: 1, Weights: [ 0.    0.    0.   -0.01  0.   -0.01  0.    0.01  0.01], Bias: 0.01\n",
            "Current Input: [1 1 1 1 1 1 1 0 0], Output: -1, Target: -1, Weights: [ 0.    0.    0.   -0.01  0.   -0.01  0.    0.01  0.01], Bias: -0.01\n",
            "Final Weights: [ 0.    0.    0.   -0.01  0.   -0.01  0.    0.01  0.01], Final Biases: [ 0.01 -0.01]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PdeO8Rd3gCX5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}